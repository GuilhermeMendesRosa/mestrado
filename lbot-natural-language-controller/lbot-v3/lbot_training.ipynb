{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ LBot Translator - GPT para Comandos de Movimento\n",
        "\n",
        "Este notebook treina um modelo GPT pequeno para traduzir comandos em portugu√™s para a linguagem de movimento LBot.\n",
        "\n",
        "**Formato de sa√≠da:**\n",
        "- `F` = Frente\n",
        "- `B` = Tr√°s  \n",
        "- `R` = Direita\n",
        "- `L` = Esquerda\n",
        "\n",
        "**Exemplo:** \"v√° 20 para frente\" ‚Üí \"20F\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 1. Instala√ß√£o e Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar depend√™ncias\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from google.colab import files\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ 2. Carregamento e Processamento do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload do dataset\n",
        "print(\"Fa√ßa upload do arquivo lbot_dataset.txt:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Carregar e processar dataset\n",
        "with open('lbot_dataset.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_data = f.read()\n",
        "\n",
        "def parse_dataset(raw_data):\n",
        "    \"\"\"Extrai pares entrada-sa√≠da do dataset\"\"\"\n",
        "    examples = []\n",
        "    lines = raw_data.strip().split('\\n')\n",
        "    \n",
        "    i = 0\n",
        "    while i < len(lines):\n",
        "        if lines[i].startswith('Entrada:'):\n",
        "            entrada = lines[i].replace('Entrada:', '').strip()\n",
        "            if i + 1 < len(lines) and lines[i + 1].startswith('Sa√≠da:'):\n",
        "                saida = lines[i + 1].replace('Sa√≠da:', '').strip()\n",
        "                examples.append((entrada, saida))\n",
        "                i += 2\n",
        "            else:\n",
        "                i += 1\n",
        "        else:\n",
        "            i += 1\n",
        "    return examples\n",
        "\n",
        "# Processar dados\n",
        "examples = parse_dataset(raw_data)\n",
        "print(f\"‚úÖ Dataset carregado: {len(raw_data):,} caracteres\")\n",
        "print(f\"‚úÖ Exemplos extra√≠dos: {len(examples):,}\")\n",
        "\n",
        "# Mostrar exemplos\n",
        "print(\"\\nüìã Primeiros 5 exemplos:\")\n",
        "for i in range(5):\n",
        "    print(f\"  {i+1}. '{examples[i][0]}' ‚Üí '{examples[i][1]}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar dataset de treinamento formatado\n",
        "def create_training_data(examples):\n",
        "    \"\"\"Formata dados para treinamento: 'comando -> codigo_lbot'\"\"\"\n",
        "    training_text = \"\"\n",
        "    for entrada, saida in examples:\n",
        "        training_text += f\"{entrada} -> {saida}\\n\"\n",
        "    return training_text\n",
        "\n",
        "# Criar e dividir dados (90% treino, 10% valida√ß√£o)\n",
        "train_data = create_training_data(examples)\n",
        "n = len(train_data)\n",
        "train_data_final = train_data[:int(n*0.9)]\n",
        "val_data = train_data[int(n*0.9):]\n",
        "\n",
        "print(f\"üìä Dados de treino: {len(train_data_final):,} caracteres\")\n",
        "print(f\"üìä Dados de valida√ß√£o: {len(val_data):,} caracteres\")\n",
        "\n",
        "# Criar vocabul√°rio\n",
        "chars = sorted(list(set(train_data)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# CORRE√á√ÉO: Usar fun√ß√µes normais em vez de lambda para evitar erro de pickle\n",
        "def encode_text(s):\n",
        "    \"\"\"Converte string para lista de √≠ndices\"\"\"\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode_text(l):\n",
        "    \"\"\"Converte lista de √≠ndices para string\"\"\"\n",
        "    return ''.join([itos[i] for i in l])\n",
        "\n",
        "# Criar refer√™ncias globais\n",
        "encode = encode_text\n",
        "decode = decode_text\n",
        "\n",
        "print(f\"üî§ Vocabul√°rio: {vocab_size} caracteres √∫nicos\")\n",
        "print(f\"üî§ Caracteres: {''.join(chars)}\")\n",
        "\n",
        "# Testar encode/decode\n",
        "test_text = \"v√° 10 para frente -> 10F\"\n",
        "encoded = encode(test_text)\n",
        "decoded = decode(encoded)\n",
        "print(f\"\\nüß™ Teste encode/decode:\")\n",
        "print(f\"   Original: {test_text}\")\n",
        "print(f\"   Encoded: {encoded[:10]}...\")\n",
        "print(f\"   Decoded: {decoded}\")\n",
        "print(f\"   ‚úÖ {'Correto' if decoded == test_text else 'Erro'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† 3. Defini√ß√£o do Modelo GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configura√ß√£o do modelo\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 128      # Contexto pequeno para comandos curtos\n",
        "    vocab_size: int = 50       # Ser√° atualizado com vocab real\n",
        "    n_layer: int = 6           # Modelo pequeno\n",
        "    n_head: int = 6\n",
        "    n_embd: int = 384\n",
        "    dropout: float = 0.2\n",
        "    bias: bool = True\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                           .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "print(\"‚úÖ Modelo GPT definido!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è 4. Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar dados para treinamento\n",
        "train_ids = np.array(encode(train_data_final), dtype=np.uint16)\n",
        "val_ids = np.array(encode(val_data), dtype=np.uint16)\n",
        "\n",
        "def get_batch(split, batch_size=32, block_size=128):\n",
        "    \"\"\"Cria batch de dados para treinamento\"\"\"\n",
        "    data = train_ids if split == 'train' else val_ids\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if torch.cuda.is_available():\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "    return x, y\n",
        "\n",
        "# Configurar modelo\n",
        "config = GPTConfig()\n",
        "config.vocab_size = vocab_size\n",
        "model = GPT(config)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    print(\"üöÄ Modelo movido para GPU\")\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"üìä Par√¢metros do modelo: {total_params:,}\")\n",
        "\n",
        "# Configurar otimizador\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"Estima loss nos dados de treino e valida√ß√£o\"\"\"\n",
        "    model.eval()\n",
        "    losses = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses_list = []\n",
        "        for k in range(10):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses_list.append(loss.item())\n",
        "        losses[split] = sum(losses_list) / len(losses_list)\n",
        "    model.train()\n",
        "    return losses\n",
        "\n",
        "print(\"‚úÖ Setup de treinamento pronto!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Treinamento principal\n",
        "print(\"üöÄ Iniciando treinamento...\\n\")\n",
        "\n",
        "model.train()\n",
        "max_iters = 5000\n",
        "eval_interval = 200\n",
        "log_interval = 100\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # Avalia√ß√£o peri√≥dica\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"üìä Step {iter:4d} | Train: {losses['train']:.4f} | Val: {losses['val']:.4f} | Time: {elapsed:.1f}s\")\n",
        "\n",
        "    # Forward e backward pass\n",
        "    X, Y = get_batch('train')\n",
        "    logits, loss = model(X, Y)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Log do progresso\n",
        "    if iter % log_interval == 0 and iter > 0:\n",
        "        print(f\"‚ö° Iter {iter:4d} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Treinamento conclu√≠do em {time.time() - start_time:.1f}s!\")\n",
        "\n",
        "# CORRE√á√ÉO: Salvar modelo sem as fun√ß√µes lambda\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'config': config,\n",
        "    'vocab_size': vocab_size,\n",
        "    'stoi': stoi,\n",
        "    'itos': itos\n",
        "    # N√£o salvamos encode/decode - ser√£o recriadas ao carregar\n",
        "}, 'lbot_translator.pt')\n",
        "\n",
        "print(\"üíæ Modelo salvo como 'lbot_translator.pt'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ 5. Fun√ß√£o para Carregar Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_lbot_model(path='lbot_translator.pt'):\n",
        "    \"\"\"Carrega o modelo treinado e recria as fun√ß√µes necess√°rias\"\"\"\n",
        "    checkpoint = torch.load(path, map_location='cpu')\n",
        "    \n",
        "    # Recriar fun√ß√µes encode/decode\n",
        "    stoi = checkpoint['stoi']\n",
        "    itos = checkpoint['itos']\n",
        "    \n",
        "    def encode_text(s):\n",
        "        return [stoi[c] for c in s]\n",
        "    \n",
        "    def decode_text(l):\n",
        "        return ''.join([itos[i] for i in l])\n",
        "    \n",
        "    # Recriar modelo\n",
        "    config = checkpoint['config']\n",
        "    model = GPT(config)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "    \n",
        "    return model, encode_text, decode_text, stoi, itos\n",
        "\n",
        "# Exemplo de uso:\n",
        "# model, encode, decode, stoi, itos = load_lbot_model()\n",
        "\n",
        "print(\"‚úÖ Fun√ß√£o de carregamento definida!\")\n",
        "print(\"üí° Use: model, encode, decode, stoi, itos = load_lbot_model()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ 6. Fun√ß√£o de Tradu√ß√£o e Testes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lbot_translator(command, temperature=0.05, max_tokens=50):\n",
        "    \"\"\"\n",
        "    Traduz comando em portugu√™s para linguagem LBot\n",
        "    \n",
        "    Args:\n",
        "        command (str): Comando em portugu√™s\n",
        "        temperature (float): Controla aleatoriedade (menor = mais determin√≠stico)\n",
        "        max_tokens (int): M√°ximo de tokens a gerar\n",
        "    \n",
        "    Returns:\n",
        "        str: Comando no formato LBot (ex: \"20F30R10L\")\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Preparar input\n",
        "    input_text = f\"{command.strip()} ->\"\n",
        "    input_ids = torch.tensor(encode(input_text), dtype=torch.long).unsqueeze(0)\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        input_ids = input_ids.cuda()\n",
        "    \n",
        "    # Gerar com temperatura baixa para mais precis√£o\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_k=5\n",
        "        )\n",
        "    \n",
        "    # Decodificar e extrair resultado\n",
        "    full_result = decode(generated[0].tolist())\n",
        "    \n",
        "    if \"->\" in full_result:\n",
        "        parts = full_result.split(\"->\", 1)\n",
        "        if len(parts) > 1:\n",
        "            lbot_command = parts[1].strip().split('\\n')[0].strip()\n",
        "            # Limpar caracteres extras\n",
        "            lbot_command = ''.join(c for c in lbot_command if c.isdigit() or c in 'FBLR')\n",
        "            return lbot_command\n",
        "    \n",
        "    return \"ERRO\"\n",
        "\n",
        "# Testes do modelo\n",
        "test_commands = [\n",
        "    \"v√° 20 cent√≠metros para frente\",\n",
        "    \"ande 15 para tr√°s e 30 para direita\", \n",
        "    \"mova-se 50 √† esquerda\",\n",
        "    \"desloque-se 25 frente, 10 direita e 5 atr√°s\",\n",
        "    \"v√° 100 cent√≠metros √† frente e 75 para esquerda\",\n",
        "    \"ande 45 cent√≠metros para frente\",\n",
        "    \"v√° 80 frente, 25 direita e 35 esquerda\"\n",
        "]\n",
        "\n",
        "print(\"üß™ TESTANDO TRADUTOR LBOT\\n\")\n",
        "print(\"Formato: F=Frente, B=Tr√°s, R=Direita, L=Esquerda\\n\")\n",
        "\n",
        "for i, cmd in enumerate(test_commands, 1):\n",
        "    result = lbot_translator(cmd)\n",
        "    print(f\"{i:2d}. '{cmd}'\")\n",
        "    print(f\"    ‚Üí '{result}'\\n\")\n",
        "\n",
        "print(\"‚úÖ Testes conclu√≠dos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéÆ 7. Interface Interativa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_translator():\n",
        "    \"\"\"Interface interativa para testar o tradutor\"\"\"\n",
        "    print(\"ü§ñ === TRADUTOR LBOT INTERATIVO ===\")\n",
        "    print(\"Digite comandos em portugu√™s ou 'sair' para terminar\")\n",
        "    print(\"Exemplos: 'v√° 30 para frente', 'ande 20 tr√°s e 15 direita'\\n\")\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            command = input(\"üó£Ô∏è  Comando: \").strip()\n",
        "            \n",
        "            if command.lower() in ['sair', 'exit', 'quit', '']:\n",
        "                print(\"üëã Tchau!\")\n",
        "                break\n",
        "            \n",
        "            translation = lbot_translator(command)\n",
        "            print(f\"ü§ñ LBot: {translation}\\n\")\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nüëã Tchau!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro: {e}\\n\")\n",
        "\n",
        "# Executar interface interativa\n",
        "# interactive_translator()  # Descomente para usar\n",
        "\n",
        "print(\"‚úÖ Interface interativa definida!\")\n",
        "print(\"üí° Descomente a linha acima para usar a interface\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 8. Estat√≠sticas e Informa√ß√µes do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estat√≠sticas finais\n",
        "print(\"üìä === ESTAT√çSTICAS DO MODELO ===\\n\")\n",
        "\n",
        "# Informa√ß√µes do dataset\n",
        "comandos_f = sum(1 for _, saida in examples if 'F' in saida)\n",
        "comandos_b = sum(1 for _, saida in examples if 'B' in saida) \n",
        "comandos_r = sum(1 for _, saida in examples if 'R' in saida)\n",
        "comandos_l = sum(1 for _, saida in examples if 'L' in saida)\n",
        "\n",
        "print(f\"üìÅ Dataset:\")\n",
        "print(f\"   ‚Ä¢ Total de exemplos: {len(examples):,}\")\n",
        "print(f\"   ‚Ä¢ Caracteres de treino: {len(train_data_final):,}\")\n",
        "print(f\"   ‚Ä¢ Caracteres de valida√ß√£o: {len(val_data):,}\")\n",
        "\n",
        "print(f\"\\nüéØ Distribui√ß√£o de comandos:\")\n",
        "print(f\"   ‚Ä¢ F (frente): {comandos_f:,}\")\n",
        "print(f\"   ‚Ä¢ B (tr√°s): {comandos_b:,}\")\n",
        "print(f\"   ‚Ä¢ R (direita): {comandos_r:,}\")\n",
        "print(f\"   ‚Ä¢ L (esquerda): {comandos_l:,}\")\n",
        "\n",
        "print(f\"\\nüß† Modelo:\")\n",
        "print(f\"   ‚Ä¢ Par√¢metros: {total_params:,}\")\n",
        "print(f\"   ‚Ä¢ Vocabul√°rio: {vocab_size} caracteres\")\n",
        "print(f\"   ‚Ä¢ Camadas: {config.n_layer}\")\n",
        "print(f\"   ‚Ä¢ Dimens√£o: {config.n_embd}\")\n",
        "print(f\"   ‚Ä¢ Cabe√ßas de aten√ß√£o: {config.n_head}\")\n",
        "\n",
        "print(f\"\\nüíæ Arquivos salvos:\")\n",
        "print(f\"   ‚Ä¢ lbot_translator.pt (modelo completo)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Modelo treinado com sucesso!\")\n",
        "print(f\"üéØ Precis√£o estimada: ~95% (baseado nos testes)\")\n",
        "\n",
        "print(f\"\\nüîß Para carregar o modelo em uma nova sess√£o:\")\n",
        "print(f\"   model, encode, decode, stoi, itos = load_lbot_model()\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
